## Acknowledgments

We would like to thank [Blake Richards](http://linclab.org/blake-richards/), [Kory Mathewson](http://korymathewson.com/), [Kyle McDonald](http://www.kylemcdonald.net), [Kai Arulkumaran](http://kaixhin.com), [Ankur Handa](https://ankurhanda.github.io), [Denny Britz](http://www.wildml.com/), [Elwin Ha](http://elwinha.com) and [Natasha Jaques](https://www.media.mit.edu/people/jaquesn/overview/) for their thoughtful feedback on this article, and for offering their valuable perspectives and insights from their areas of expertise.

The interative demos in this article were all built using [p5.js](https://p5js.org/). Deploying all of these machine learning models in a web browser was made possible with [deeplearn.js](https://deeplearnjs.org/), a hardware-accelerated machine learning framework for the browser, developed by the [People+AI Research Initiative](https://ai.google/pair) (PAIR) team at Google. A special thanks goes to Nikhil Thorat and Daniel Smilkov for their support.

We would like to thank Chris Olah and the rest of the Distill editorial team for their valuable feedback and generous editorial support, in addition to supporting the use of their [distill.pub](https://distill.pub) technology.

We would to extend our thanks to Alex Graves, Douglas Eck, Mike Schuster, Rajat Monga, Vincent Vanhoucke, Jeff Dean and the Google Brain team for helpful feedback and for encouraging us to explore this area of research.

Any errors here are our own and do not reflect opinions of our proofreaders and colleagues. If you see mistakes or want to suggest changes, feel free to contribute feedback by participating in the discussion [forum](https://github.com/worldmodels/worldmodels.github.io/issues) for this article.

The experiments in this article were performed on both a P100 GPU and a 64-core CPU Ubuntu Linux virtual machine provided by [Google Cloud Platform](https://cloud.google.com/), using [TensorFlow](https://www.tensorflow.org/) and [OpenAI Gym](https://github.com/openai/gym).

<h3 id="citation">Citation</h3>

For attribution in academic contexts, please cite this work as

<pre class="citation short">Ha and Schmidhuber, "Recurrent World Models Facilitate Policy Evolution", 2018.</pre>

BibTeX citation

<pre class="citation long">@incollection{ha2018worldmodels,
  title = {Recurrent World Models Facilitate Policy Evolution},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  booktitle = {Advances in Neural Information Processing Systems 31},
  pages = {2451--2463},
  year = {2018},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper/7512-recurrent-world-models-facilitate-policy-evolution},
  note = "\url{https://worldmodels.github.io}",
}</pre>

### Open Source Code

The instructions to reproduce the experiments in this work is available [here](http://blog.otoro.net/2018/06/09/world-models-experiments/).

### Reuse

Diagrams and text are licensed under Creative Commons Attribution [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) with the [source available on GitHub](https://github.com/worldmodels/worldmodels.github.io), unless noted otherwise. The figures that have been reused from other sources don’t fall under this license and can be recognized by the citations in their caption.

<h2 id="appendix">Appendix</h2>

In this section we will describe in more details the models and training methods used in this work.

### Variational Autoencoder

We trained a Convolutional Variational Autoencoder (ConvVAE) model as our agent's V. Unlike vanilla autoencoders, enforcing a Gaussian prior over the latent vector $z_t$ also limits the amount of information capacity for compressing each frame, but this Gaussian prior also makes the world model more robust to unrealistic $z_t \in \mathbb{R}^{N_z}$ vectors generated by M.

In the following diagram, we describe the shape of our tensor at each layer of the ConvVAE and also describe the details of each layer:

<div style="text-align: center;">
<img class="b-lazy" src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/conv_vae_label.svg" style="display: block; margin: auto; width: 50%;"/>
<figcaption>Convolutional Variational Autoencoder</figcaption>
</div>

Our latent vector $z_t$ is sampled from a factored Gaussian distribution $N(\mu_t, \sigma_t^2 I)$, with mean $\mu_t\in \mathbb{R}^{N_z}$ and diagonal variance $\sigma_t^2 \in \mathbb{R}^{N_z}$. As the environment may give us observations as high dimensional pixel images, we first resize each image to 64x64 pixels and use this resized image as V's observation. Each pixel is stored as three floating point values between 0 and 1 to represent each of the RGB channels. The ConvVAE takes in this 64x64x3 input tensor and passes it through 4 convolutional layers to encode it into low dimension vectors $\mu_t$ and $\sigma_t$. In the Car Racing task, $N_z$ is 32 while for the Doom task $N_z$ is 64. The latent vector $z_t$ is passed through 4 of deconvolution layers used to decode and reconstruct the image.

Each convolution and deconvolution layer uses a stride of 2. The layers are indicated in the diagram in *Italics* as *Activation-type Output Channels x Filter Size*. All convolutional and deconvolutional layers use relu activations except for the output layer as we need the output to be between 0 and 1. We trained the model for 1 epoch over the data collected from a random policy, using $L^2$ distance between the input image and the reconstruction to quantify the reconstruction loss we optimize for, in addition to KL loss.

### Mixture Density Network + Recurrent Neural Network

For the M Model, we use an <dt-cite key="lstm">LSTM</dt-cite> recurrent neural network combined with a Mixture Density Network <dt-cite key="bishop,mdntf"></dt-cite> as the output layer. We use this network to model the probability distribution of the next $z$ in the next time step as a Mixture of Gaussian distribution. This approach is very similar to Graves' *Generating Sequences with RNNs* <dt-cite key="graves_rnn"></dt-cite> in the Unconditional Handwriting Generation section and also the decoder-only section of *SketchRNN* <dt-cite key="sketchrnn"></dt-cite>. The only difference in the approach used is that we did not model the correlation parameter between each element of $z$, and instead had the MDN-RNN output a diagonal covariance matrix of a factored Gaussian distribution.

To implement M, we use an LSTM <dt-cite key="lstm">LSTM</dt-cite> recurrent neural network combined with a Mixture Density Network <dt-cite key="bishop,mdntf"></dt-cite> as the output layer, as illustrated in figure below:

<div style="text-align: center;">
<img class="b-lazy" src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/mdn_rnn.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>MDN-RNN <dt-cite key="sketchrnn"></dt-cite></figcaption>
</div>

We use this network to model the probability distribution of $z_t$ as a Mixture of Gaussian distribution. This approach is very similar to previous work <dt-cite key="graves_rnn"></dt-cite> in the Unconditional Handwriting Generation section and also the decoder-only section of *SketchRNN* <dt-cite key="sketchrnn"></dt-cite>. The only difference is that we did not model the correlation parameter between each element of $z_t$, and instead had the MDN-RNN output a diagonal covariance matrix of a factored Gaussian distribution.

Unlike the handwriting and sketch generation works, rather than using the MDN-RNN to model the probability density function (pdf) of the next pen stroke, we model instead the pdf of the next latent vector $z_t$. We would sample from this pdf at each time step to generate the environments. In the Doom task, we also use the MDN-RNN to predict the probability of whether the agent has died in this frame. If that probability is above 50%, then we set <code>done</code> to be <code>True</code> in the virtual environment. Given that death is a low probability event at each time step, we find the cutoff approach to be more stable compared to sampling from the Bernoulli distribution.

The MDN-RNNs were trained for 20 epochs on the data collected from a random policy agent. In the Car Racing task, the LSTM used 256 hidden units, in the Doom task 512 hidden units. In both tasks, we used 5 Gaussian mixtures, but unlike <dt-cite key="graves_rnn,sketchrnn"></dt-cite>, we did not model the correlation parameters, hence $z_t$ is sampled from a factored mixture of Gaussian distributions.

When training the MDN-RNN using teacher forcing from the recorded data, we store a pre-computed set of $\mu_t$ and $\sigma_t$ for each of the frames, and sample an input $z_t \sim N(\mu_t, \sigma_t^2 I)$ each time we construct a training batch, to prevent overfitting our MDN-RNN to a specific sampled $z_t$.

### Controller

For both environments, we applied $\tanh$ nonlinearities to clip and bound the action space to the appropriate ranges. For instance, in the Car Racing task, the steering wheel has a range from -1.0 to 1.0, the acceleration pedal from 0.0 to 1.0, and the brakes from 0.0 to 1.0. In the Doom environment, we converted the discrete actions into a continuous action space between -1.0 to 1.0, and divided this range into thirds to indicate whether the agent is moving left, staying where it is, or moving to the right. We would give C a feature vector as its input, consisting of $z_t$ and the hidden state of the MDN-RNN. In the Car Racing task, this hidden state is the output vector $h_t \in \mathbb{R}^{N_h}$ of the LSTM, while for the Doom task it is both the cell vector $c_t \in \mathbb{R}^{N_h}$ and the output vector $h_t$ of the LSTM.

### Evolution Strategies

We used <dt-cite key="cmaes">Covariance-Matrix Adaptation Evolution Strategy (CMA-ES)</dt-cite> to evolve C's weights. Following the approach described in <dt-cite key="stablees">Evolving Stable Strategies</dt-cite>, we used a population size of 64, and had each agent perform the task 16 times with different initial random seeds. The agent's fitness value is the *average cumulative reward* of the 16 random rollouts. The figure below charts the best performer, worst performer, and mean fitness of the population of 64 agents at each generation:

<div style="text-align: center;">
<img class="b-lazy" src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/carracing.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>Training of <dt-cite key="carracing_v0">CarRacing-v0</dt-cite></figcaption>
</div>

Since the requirement of this environment is to have an agent achieve an average score above 900 over 100 random rollouts, we took the best performing agent at the end of every 25 generations, and tested it over 1024 random rollout scenarios to record this average on the red line. After 1800 generations, an agent was able to achieve an average score of 900.46 over 1024 random rollouts. We used 1024 random rollouts rather than 100 because each process of the 64 core machine had been configured to run 16 times already, effectively using a full generation of compute after every 25 generations to evaluate the best agent 1024 times. In the figure below, we plot the results of same agent evaluated over 100 rollouts:

<div style="text-align: center;">
<img class="b-lazy" src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/carracing_histogram.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>Histogram of cumulative rewards.<br/>Average score is 906 ± 21.</figcaption>
</div>

We also experimented with an agent that has access to only the $z_t$ vector from the VAE, but not  the RNN's hidden states. We tried 2 variations, where in the first variation, C maps $z_t$ directly to the action space $a_t$. In second variation, we attempted to add a hidden layer with 40 $tanh$ activations between $z_t$ and $a_t$, increasing the number of model parameters of C to 1443, making it more comparable with the original setup. These results are shown in the two figures below:

<div style="text-align: center;">
<img class="b-lazy" src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/carracing_histogram_z.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>When agent sees only <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, average score is 632 ± 251.</figcaption>
</div>

<div style="text-align: center;">
<img class="b-lazy" src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/carracing_histogram_z_hidden.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>When agent sees only <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.04398em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>, with a hidden layer, average score is 788 ± 141.</figcaption>
</div>

### DoomRNN

We conducted a similar experiment on the generated Doom environment we called *DoomRNN*. Please note that we did not attempt to train our agent on the actual VizDoom environment, but only used VizDoom for the purpose of collecting training data using a random policy. *DoomRNN* is more computationally efficient compared to VizDoom as it only operates in latent space without the need to render an image at each time step, and we do not need to run the actual Doom game engine.

In our virtual DoomRNN environment we increased the temperature slightly and used $\tau=1.15$ to make the agent learn in a more challenging environment. The best agent managed to obtain an average score of 959 over 1024 random rollouts. This is the highest score of the red line in the figure below:

<div style="text-align: center;">
<img class="b-lazy" src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/doomrnn.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>Training of DoomRNN</figcaption>
</div>

This same agent achieved an average score of 1092 $\pm$ 556 over 100 random rollouts when deployed to the actual <dt-cite key="takecover">DoomTakeCover-v0</dt-cite> environment, as shown in the figure below:

<div style="text-align: center;">
<img class="b-lazy" src=data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw== data-src="assets/doomtakecover_histogram.svg" style="display: block; margin: auto; width: 100%;"/>
<figcaption>Histogram of time steps survived in the actual environment over 100 consecutive trials.<br/>Average score is 1092 ± 556.</figcaption>
</div>